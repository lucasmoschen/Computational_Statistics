\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[right=2.5cm, left=2.5cm, top=2.5cm, bottom=2.5cm]{geometry} 
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumerate}

% no indentation
%\usepackage{setspace}
%\setlength{\parindent}{0in}

\usepackage{graphicx} 
\usepackage{float}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{mathtools}
\usepackage{amssymb, amsthm}

% headers
\usepackage{fancyhdr}
\usepackage{xurl}
\usepackage{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proper definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\var}{\operatorname{Var}}
\newcommand{\unif}{\operatorname{Unif}}
\newcommand{\bin}{\operatorname{Bin}}
\newcommand{\ev}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}

\newtheorem*{aff}{Afirmação}

\newtheorem{exercise}{Exercício}

\theoremstyle{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header (and Footer)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy} 
\fancyhf{}

\lhead{\footnotesize CE: Problem sheet 2}
\rhead{\footnotesize Prof. Luiz} 
\cfoot{\footnotesize \thepage} 


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title section of the document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} 

\begin{tabular*}{0.95\textwidth}{l @{\extracolsep{\fill}} r} 
    {\large \bf Computational statistics 2021.2} &  \\
    School of Applied Mathematics, Fundação Getulio Vargas &  \\
    Professor Luiz Max de Carvalho  &  \\ 
    \hline \\
\end{tabular*} 
\vspace*{0.3cm} 

\begin{center}
	{\Large \bf Problem sheet 2} 
	\vspace{2mm}
    \\
	{\bf Lucas Machado Moschen}	
\end{center}  
\vspace{0.4cm}

\begin{exercise}
    (Monte Carlo for Gaussian) 

\noindent Let us consider the normal multivariate density on $\R^d$ with identity
    covariance, that is
    $$
    \pi(x) = \frac{1}{(2\pi)^{d/2}} \exp\left\{-\frac{1}{2}x^Tx\right\}.
    $$
\end{exercise}

\begin{enumerate}
    \item {\it (Cameron-Martin formula). Show that for any $\theta \in  \R^d$ 
    and function $\phi : \R^d \to \R$}
    $$
    \ev[\phi(X)] = \ev\left[\phi(X + \theta) \exp\left(-\frac{1}{2}\theta^T\theta - 
    \theta^TX\right)\right].
    $$

    Let $\phi$ be any measurable function and $\theta \in \R^d$. Denote $I_2$ 
    the quantity in the right of the equation. Then, 
    \begin{equation*}
        \begin{split}
            I_2 &= \int_{\R^d} \phi(x+\theta)\exp\left(-\frac{1}{2}\theta^T\theta - 
    \theta^Tx\right) \pi(x) \, dx \\
        &= \int_{\R^d} \phi(x+\theta)\exp\left(-\frac{1}{2}\theta^T\theta - 
        \theta^Tx\right) \pi(x) \, dx \\
        &= \int_{\R^d} \frac{1}{(2\pi)^{d/2}}\phi(x+\theta)
        \exp\left(-\frac{1}{2}(x + \theta)^T(x+ \theta)\right) \, dx \\
        &= \int_{\R^d} \frac{1}{(2\pi)^{d/2}}\phi(y)
        \exp\left(-\frac{1}{2}y^Ty\right) \, dx \\
        &= \ev[\phi(X)]. 
        \end{split} 
    \end{equation*}

    \item {\it It follows directly from the Cameron-Martin formula 
    and the strong law of large numbers that, for
    independent $X_1 , \dots, X_n \sim \pi$, the estimator}
    $$
    \hat{I}_n(\theta) = \frac{1}{n}\sum_{i=1}^n \phi(X_n + \theta)
    \exp\left(-\frac{1}{2}\theta^T\theta - \theta^TX_i\right)
    $$
    {\it of $\ev[\phi(X)]$ is strongly consistent for any $\theta \in \R^d$ 
    such that}
    $$\ev\left[|\phi(X + \theta) \exp\left(-\frac{1}{2}\theta^T\theta - 
    \theta^TX\right)|\right] < +\infty.$$
    {\it The case $\theta = 0$ corresponds to the usual Monte Carlo 
    estimate. The variance of $\hat{I}_n(\theta)$ is given by 
    $\sigma^2(\theta)/n$ where}
    $$
    \sigma^2(\theta) = \var\left(\phi(X + \theta) \exp\left(-\frac{1}{2}\theta^T\theta - 
    \theta^TX\right)\right).
    $$
    {\it We assume in the sequel that $\sigma^2(\theta) < \infty$ for 
    any $\theta$. Show that}
    $$
    \sigma^2(\theta) = \ev\left[\phi^2(X) \exp\left(-\frac{1}{2}X^TX +
    \frac{1}{2}(X-\theta)^T(X-\theta)\right)\right] - (\ev[\phi(X)])^2
    $$

    Let $\sigma^2(\theta) = \var(Y) = \ev[Y^2] - (\ev[Y])^2$ to 
    simplify the writing. We already know that $\ev[Y] = \ev[\phi(X)]$
    by the last exercise. Therefore, it remains to prove that 
    $$
    \ev[Y^2] =  \ev\left[\phi^2(X) \exp\left(-\frac{1}{2}X^TX +
    \frac{1}{2}(X-\theta)^T(X-\theta)\right)\right].
    $$

    For that, 
    \begin{equation*}
        \begin{split}
            \ev[Y^2] &= \int_{\R^d} \phi^2(x+\theta)\exp\left(-\theta^T\theta - 
            2\theta^Tx\right) \pi(x) \, dx \\
            &= \int_{\R^d} \frac{1}{(2\pi)^{d/2}}\phi^2(x+\theta)\exp\left(-\theta^T\theta - 
            2\theta^Tx - \frac{1}{2}x^Tx\right) \, dx \\
            &= \int_{\R^d} \frac{1}{(2\pi)^{d/2}}\phi^2(x+\theta)\exp\left(-(x + \theta)^T(x + \theta)
            + \frac{1}{2}x^Tx\right) \, dx \\
            &= \int_{\R^d} \frac{1}{(2\pi)^{d/2}}\phi^2(y)\exp\left(-y^Ty
            + \frac{1}{2}(y - \theta)^T(y - \theta)\right) \, dx \\
            &= \ev\left[\phi^2(X) \exp\left(-X^TX +
            \frac{1}{2}(X-\theta)^T(X-\theta)\right)\right],
        \end{split}
    \end{equation*}
    as we wanted to prove. 

    \item {\it A twice differentiable function $f : \R^d \to \R$ is 
    strictly convex if $\nabla^2 f (\theta)$ (called the Hessian of 
    $f$) is a positive definite matrix for any $\theta \in \R^d$. 
    Deduce from the expression of $\sigma^2(\theta)$ given in 
    (2) that the function $\theta \to \sigma^2(\theta)$ is strictly 
    convex.}

    For that, we will use the derived expression in the last exercise
    and we differentiate under the expected value using the Leibniz 
    Rule. Then, 

    \begin{equation*}
        \nabla_{\theta} \sigma^2(\theta) = \ev\left[\phi^2(X)(\theta-X)\exp\left(-X^TX +
        \frac{1}{2}(X-\theta)^T(X-\theta)\right)\right]
    \end{equation*}
    and 
    \begin{equation*}
        \begin{split}
            \nabla^2_{\theta} \sigma^2(\theta) &= \ev\left[\phi^2(X)\exp\left(-X^TX +
        \frac{1}{2}(X-\theta)^T(X-\theta)\right)((\theta - X)^T(\theta - X) + 1)\right], 
        \end{split}
    \end{equation*}
    which is clearly positive definite since $(\theta -X)^T(\theta-X)$ 
    is semi definite positive.

    \item Show that the minimum of $\theta \to \sigma^2(\theta)$ is 
    reached at $\theta^{*}$ such that
    $$
    \ev[\phi^2(X)(\theta^* - X)\exp(-\theta^{*T}X)] = 0.
    $$

    Since $\sigma^2(\theta)$ is differentiable, its critical points are 
    the solution of $\nabla_\theta \sigma^2(\theta) = 0$, 
    \begin{equation*}
        \begin{split}
            &\ev\left[\phi^2(X)(\theta-X)\exp\left(-X^TX +
        \frac{1}{2}(X-\theta)^T(X-\theta)\right)\right] = 0 \\
        &\implies \ev\left[\phi^2(X)(\theta-X)\exp\left(-\frac{1}{2}X^TX
        -\theta^TX\right)\right] = 0, 
        \end{split}
    \end{equation*}  
    since $e^{\theta^T\theta/2}$ is a positive constant. Since the function 
    is strictly convex, we already know that there is only one minimal 
    and it occurs when the above expression is zero. 
\end{enumerate}

\begin{exercise}
    
\end{exercise}

\begin{exercise}
    
\end{exercise}

\begin{exercise}
    (Gibbs Sampler) 
    Suppose that we wish to use the Gibbs sampler on
    $$
    \pi(x,y) \propto \exp\left(-\frac{1}{2}(x-1)^2(y-2)^2\right).
    $$
\end{exercise}

\begin{enumerate}
    \item {\it Write down the two ``full'' conditional distributions associated to $\pi(x, y)$.}
    \begin{equation*}
        \begin{split}
            \pi(x) &= \int_{y \in \R} \pi(x,y) \, dy\\
            &= \int_{y \in \R} c\exp\left(-\frac{1}{2}(x-1)^2(y-2)^2\right)\,dy \\
            &= c\int_{y \in \R} \exp\left(-\frac{1}{2}(x-1)^2(y-2)^2\right)\,dy \\
            &= c\int_{y \in \R} \exp\left(-\frac{1}{2/(x-1)^2} (y-2)^2\right)\,dy \\
            &= c\sqrt{2\pi}\frac{1}{|x - 1|}
        \end{split}
    \end{equation*}
    since setting $\sigma^2 = 1/(x-1)^2$, the integrand is the kernel of a
    normal distribution with mean $2$ and variance $\sigma^2$. Therefore, its
    integral is the normalization constant. With the same reasoning, we have
    that 
    $$
    \pi(y) = c\sqrt{2\pi}\frac{1}{|y-2|}.
    $$
    Then, we have that 
    $$
    \pi(x|y) = \frac{\pi(x,y)}{\pi(y)} = \frac{1}{\sqrt{2\pi}}|y-2|\exp\left(-\frac{1}{2}(x-1)^2(y-2)^2\right) 
    $$
    and 
    $$
    \pi(y|x) = \frac{\pi(x,y)}{\pi(x)} = \frac{1}{\sqrt{2\pi}}|x-1|\exp\left(-\frac{1}{2}(x-1)^2(y-2)^2\right),
    $$
    which implies that 
    $$
    X \mid Y = y \sim \operatorname{Normal}(1, |y-2|^2)
    $$
    and 
    $$
    Y \mid X = x \sim \operatorname{Normal}(2, |x-1|^2).
    $$ 
    
    \item {\it Does the resulting Gibbs sampler make any sense?}
    
    The problem with that Gibbs sampler is that the samples $(x^n, y^n)$
    converges to $(1,2)$ when $n$ is sufficiently high. This happens because
    the variances are very low in the region of greater mass. Therefore, even
    if the initial points are far from the mode, the sampling will explore it
    and wheen it happens, it will get stuck. 
\end{enumerate}

\begin{exercise}
    (Gibbs Sampler)
    For $i = 1, \dots , T$ consider $Z_i = X_i + Y_i$ with independent $X_i,
    Y_i$ such that
    $$
    X_i \sim \bin(m_i , \theta_1), Y_i \sim \bin(n_i, \theta_2).
    $$
\end{exercise}

\begin{enumerate}
    \item {\it We assume $0 \le z_i \le m_i + n_i$ for $i = 1, \dots , T$. We
    observe $z_i$ for $i = 1,\dots, T$ and the $n_i , m_i$ for
    $i = 1, \dots, T$ are given. Give the expression of the likelihood
    function $p(z_1, \dots, z_T| \theta_1, \theta_2)$.}
    
    Supposing conditionally independent samples, we have that 
    $$
    p(z_1, \dots, z_T \mid \theta_1, \theta_2) = \prod_{i=1}^T p(z_i\mid \theta_1, \theta_2)
    $$
    Note that 
    \begin{equation*}
        \begin{split}
            \pr(Z_i = X_i + Y_i = k) &= \sum_{j=0}^k \pr(X_i = j)\pr(Y_i = k-j) \\
            &= \sum_{j=0}^k \binom{m_i}{j}\binom{n_i}{j} (\theta_1-\theta_1\theta_2)^j(\theta_2-\theta_1\theta_2)^{k-j}. 
        \end{split}
    \end{equation*}
    Therefore, 
    $$
    p(z_1, \dots, z_T \mid \theta_1, \theta_2) = \prod_{i=1}^T \sum_{j=0}^{z_i} \binom{m_i}{j}\binom{n_i}{j} (\theta_1-\theta_1\theta_2)^j(\theta_2-\theta_1\theta_2)^{z_i-j}.
    $$

    \item {\it Assume we set independent uniform priors $\theta_1 \sim
    \unif[0,1], \theta_2 \sim \unif[0,1]$. Propose a Gibbs sampler to sample from $p( \theta_1 , \theta_2 \mid z_1 , \dots, z_T)$.}
    
    We know that 
    $$
    p( \theta_1 , \theta_2 \mid z_1 , \dots, z_T) \propto p(z_1, \dots, z_T \mid \theta_1, \theta_2).    
    $$
    and we want to specify 
    $$
    p(\theta_1^t \mid \theta_2^{t-1}, z_1, \dots, z_T) 
    $$
    and
    $$
    p(\theta_2^t \mid \theta_1^{t}, z_1, \dots, z_T).
    $$ 
    Since we do not know how to sample from these distributions, we have to
    introduce auxiliary variables to help out. Let $X_1, \dots, X_T$ and $Y_1,
    \dots, Y_T$ be the variables introduced in the exercise. We denote
    $X_{1:T} = X_1, \dots, X_T$. Then 
    $$ 
    \pr(\theta_1 \mid \theta_2, X_{1:T}, Y_{1:T}, Z_{1:T}) = \pr(\theta_1 \mid X_{1:T}),  
    $$
    since given $X_{1:T}$, we have that $\theta_1$ is independent of the other
    random variables. Analogously,  
    $$ 
    \pr(\theta_2 \mid \theta_1, X_{1:T}, Y_{1:T}, Z_{1:T}) = \pr(\theta_2 \mid Y_{1:T}).  
    $$
    Given that $\{X_i \mid \theta_1\}_{i=1}^T$ are independent and have
    binomial distribution, and $\theta_1$ has Beta distribution with
    parameters $\alpha = 1$ and $\beta = 1$, we know that, 
    $$
    \theta_1 \mid X_{1:T} = x_{1:T} \sim \operatorname{Beta}\left(1 + \sum_{i=1}^T x_i, 1 + \sum_{i=1}^T m_i - x_i\right)
    $$
    and 
    $$
    \theta_2 \mid Y_{1:T} = y_{1:T} \sim \operatorname{Beta}\left(1 + \sum_{i=1}^T y_i, 1 + \sum_{i=1}^T n_i - y_i\right).
    $$
    It remains to sample from 
    $$
    X_{1:T}, Y_{1:T} \mid \theta_1, \theta_2, Z_{1:T}.
    $$
    Note that 
    \begin{equation*}
        \begin{split}
            \pr(X_{1:T} = x_{1:T}, Y_{1:T} = y_{1:T} \mid \theta_1, \theta_2, Z_{1:T}) &= \pr(X_{1:T} = x_{1:T} \mid \theta_1, \theta_2, Z_{1:T},  Y_{1:T}) \\
            &\times \pr(Y_{1:T} = y_{1:T} \mid \theta_1, \theta_2, Z_{1:T}). \\
        \end{split}
    \end{equation*}
    The first term is $1_{\{x_{1:T} = Z_{1:T} - Y_{1:T}\}}$, while the second
    can be writing as follows: 
    \begin{equation*}
        \begin{split}
            \pr(Y_{1:T} = y_{1:T} \mid \theta_1, \theta_2, Z_{1:T}) &= \prod_{i=1}^T \pr(Y_i = y_i \mid \theta_1, \theta_2, Z_i) \\
            &= \prod_{i=1}^T \bin(y_i; n_i, \theta_2) \cdot \bin(z_i - y_i; m_i, \theta_1).
        \end{split}
    \end{equation*}
    Since this distribution has finite support, one can calculate its
    constant. Therefore, we have defined our Gibbs sampler. 
    
\end{enumerate}

% \bibliographystyle{apalike}
% \bibliography{../stat_comp}

\end{document}          
