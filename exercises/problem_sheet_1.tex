\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[right=2.5cm, left=2.5cm, top=2.5cm, bottom=2.5cm]{geometry} 
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumerate}

% no indentation
%\usepackage{setspace}
%\setlength{\parindent}{0in}

\usepackage{graphicx} 
\usepackage{float}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{mathtools}
\usepackage{amssymb, amsthm}

% headers
\usepackage{fancyhdr}
\usepackage{xurl}
\usepackage{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proper definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\var}{\operatorname{Var}}
\newcommand{\unif}{\operatorname{Unif}}
\newcommand{\ev}{\mathbb{E}}
\newcommand{\pr}{\mathbb{P}}

\newtheorem*{aff}{Afirmação}

\newtheorem{exercise}{Exercício}

\theoremstyle{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Header (and Footer)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy} 
\fancyhf{}

\lhead{\footnotesize CE: Problem sheet 1}
\rhead{\footnotesize Prof. Luiz} 
\cfoot{\footnotesize \thepage} 


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title section of the document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty} 

\begin{tabular*}{0.95\textwidth}{l @{\extracolsep{\fill}} r} 
    {\large \bf Computational statistics 2021.2} &  \\
    School of Applied Mathematics, Fundação Getulio Vargas &  \\
    Professor Luiz Max de Carvalho  &  \\ 
    \hline \\
\end{tabular*} 
\vspace*{0.3cm} 

\begin{center}
	{\Large \bf Problem sheet 1} 
	\vspace{2mm}
    \\
	{\bf Lucas Machado Moschen}	
\end{center}  
\vspace{0.4cm}

\begin{exercise}
    (Inversion and Rejection)
\end{exercise}

\begin{enumerate}
    \item {\it Let $Y \sim \operatorname{Exp}(\lambda)$ and let $a > 0$. We consider the variable after restricting its support to be $[a, +\infty)$.
    That is, let $X = Y_{|Y \ge a}$, i.e. $X$ has the law of $Y$ conditionally on being in $[a, +\infty)$. Calculate
    $F_X(x)$, the cumulative distribution function of $X$, and
    $F^{-1}_X(u)$, the quantil function of $X$. Describe an algorithm to
    simulate $X$ from $U \sim \unif[0,1]$.}

    If $x \ge a$, we have that
    \begin{equation*}
        \begin{split}
            F_X(x) &= \pr(Y \le x \mid Y \ge a) \\
            &= \frac{\pr(Y \in [a, x])}{\pr(Y \ge a)} \\
            &= \frac{1 - e^{-\lambda x} - (1 - e^{-\lambda a})}{e^{-\lambda a}} \\
            &= 1 - e^{-\lambda(x - a)},
        \end{split}
    \end{equation*}
    otherwise, $F_X(x) = 0$. Let $u = 1 - e^{-\lambda(x - a)}$. Inverting this
    function, we get that 
    $$
    F_X^{-1}(u) = a-\frac{\log(1 - u)}{\lambda}.
    $$

    A simple algorithm is the following 
    \begin{enumerate}[(i)]
        \item Let $U \sim \unif[0,1]$. 
        \item Define $X = F_X^{-1}(U)$. Then $X$ has the desired distribution
        by the inversion method. 
    \end{enumerate}
    
    \item {\it Let $a$ and $b$ be given, with $a < b$. Show that we can simulate $X = Y_
    {|a \le Y \le b}$ from $U \sim \unif[0,1]$ using
    $$
    X= F_Y^{-1}(F_Y(a)(1 - U) + F_Y(b)U),
    $$
    i.e. show that if $X$ is given by the formula above, then $\pr(X \le
    x) = \pr(Y \le x\mid a \le Y \le b)$. Apply
    the formula to simulate an exponential random variable conditioned to
    be greater than a, as in the previous question.}

    Using the properties of the (generalized) inverse and some affine
    transformations, note that
    \begin{equation*}
        \begin{split}
            \pr(X \le x) &= \pr(F_Y^{-1}(F_Y(a)(1 - U) + F_Y(b)U) \le x) \\
            &= \pr(F_Y(a)(1 - U) + F_Y(b)U \le F_Y(x)) \\
            &= \pr(U(F_Y(b) - F_Y(a)) \le F_Y(x) - F_Y(a)) \\
            &= \pr\left(U \le \frac{F_Y(x) - F_Y(a)}{F_Y(b) - F_Y(a)}\right) = \frac{F_Y(x) - F_Y(a)}{F_Y(b) - F_Y(a)}.
        \end{split}
    \end{equation*}
    However, 
    $$\frac{F_Y(x) - F_Y(a)}{F_Y(b) - F_Y(a)} = \frac{\pr(Y \le x) - \pr(Y \le
    a)}{\pr(Y \le b) - \pr(Y \le a)} = \pr(Y \le x \mid Y \in [a,b]),$$
    what concludes that $X$ has the same distribution of $F_Y^{-1}(F_Y(a)(1 -
    U) + F_Y(b)U)$. 

    Taking $b = +\infty$, we can simulate $U \sim \unif[0,1]$ and use 
    $$X = F_Y^{-1}(F_Y(a)(1 - U) + U).$$

    \item {\it Here is a simple algorithm to simulate $X = Y_{|Y > a}$ for $Y
    \sim \operatorname{Exp}(\lambda)$:}

    \begin{enumerate}
        \item {\it Let $Y \sim \operatorname{Exp}(\lambda)$. Simulate $Y = y$.}
        \item {\it If $Y > a$ then stop and return $X = y$, and otherwise, start again at step (a).}
    \end{enumerate}

    {\it Show that this is just a rejection algorithm, by writing the proposal
    and target densities $\pi$ and $q$, as well as the bound $M = \max_x
    \pi(x)/q(x)$. Calculate the expected number of trials to the first
    acceptance. Why is inversion to be preferred for $a \gg 1/\lambda$?}

    The target density $\pi(x) = \frac{d}{dx}F_X(x) = \lambda
    e^{-\lambda(x-a)}1_{\{x \ge a\}}$ is the density of $X$, while the proposal density
    is the exponential $q(x) = \lambda e^{-\lambda x}1_{\{x \ge 0\}}$. Therefore, the bound
    is 
    $$
    M = \sup_{x \ge 0} \frac{\pi(x)}{q(x)} = \sup_{\{x \ge a\}} e^{\lambda a} = e^{\lambda a}. 
    $$
    The probability of accepting $X = y$ is 
    $$
    \alpha(y) = \frac{\pi(y)}{Mq(y)} = \begin{cases}
        0, &\text{ if } y \le a \\
        1, &\text{ if } y > a.
    \end{cases}.
    $$
    This is only the rejection sampling algorithm. Let $N$ be the number os
    trials to the first acceptance.  We already know that $N$ is geometrically
    distributed with parameter $M^{-1} = e^{-\lambda a}$. In our case, this is
    easy to see, because, 
    $$
    \pr(N > n) = \pr(Y \le a)^n = (1 - e^{-\lambda a})^n.
    $$
    We conclude that $\ev[N] = e^{\lambda a}$. When $a \gg 1/\lambda$, we have
    that $\ev[N] \gg e$ and several trials are rejected until a desired
    sample come. In that case, is much simpler to use the inversion method.  
\end{enumerate}

\begin{exercise}
    (Rejection)
    Consider the following ``squeeze'' rejection algorithm for sampling from a
    distribution with density $\pi(x) = \tilde{\pi}(x)/Z_{\pi}$ on a state
    space $\mathbb{X}$  such that
    $$
    h(x) \le \tilde{\pi}(x) \le M\tilde{q}(x)
    $$
    where $h$ is a non-negative function, $M > 0$ and $q(x) =
    \tilde{q}(x)/Z_q$ is the density of a distribution that we can easily
    sample from. The algorithm proceeds as follows.
    \begin{enumerate}[(a)]
        \item Draw independently $X \sim q, U \sim \unif[0,1]$.
        \item Accept $X$ if $U \le h(X)/(M\tilde{q}(X))$.
        \item If $X$ was not accepted, draw an independent $V \sim \unif[0,1]$
        and accept $X$ if 
        $$
        V \le \frac{\tilde{\pi}(X) - h(X)}{M\tilde{q}(X) - h(X)}.
        $$
    \end{enumerate}
\end{exercise}

\begin{enumerate}
    \item {\it Show that the probability of accepting a proposed $X = x$ in either step (b) or (c) is}
    $$\frac{\tilde{\pi}(x)}{M\tilde{q}(x)}.$$
    \begin{equation*}
        \begin{split}
            \pr&(\text{Accept } X \mid X = x) = \\
            &= \pr(U \le h(x) / (M\tilde{q}(x))) 
            + \pr\left(U > h(x) / (M\tilde{q}(x))\right)\pr\left(V \le \frac{\tilde{\pi}(x) - h(x)}{M\tilde{q}(x) - h(x)}\right) \\
            &= \int_0^{h(x) / (M\tilde{q}(x))} \, du  + \left(1 - \int_0^{h(x) / (M\tilde{q}(x))} \, du\right)\int_0^{(\tilde{\pi}(x) - h(x)) / (M\tilde{q}(x) - h(x)} q(x) \, du \\ 
            &= \frac{h(x)}{M \tilde{q}(x)} + \left(1 - \frac{h(x)}{M \tilde{q}(x)}\right)\left(\frac{\tilde{\pi}(x) - h(x)}{M\tilde{q}(x) - h(x)}\right) \\
            &= \frac{h(x)(M\tilde{q}(x) - h(x)) + M\tilde{q}(x)(\tilde{\pi}(x) - h(x)) - h(x)(\tilde{\pi}(x) - h(x))}{M\tilde{q}(x)(M\tilde{q}(x) - h(x))}\\
            &= \frac{\tilde{\pi}(x)(M\tilde{q}(x) - h(x))}{M\tilde{q}(x)(M\tilde{q}(x) - h(x))} \\
            &= \frac{\tilde{\pi}(x)}{M\tilde{q}(x)}. 
        \end{split}
    \end{equation*}

    \item {\it Deduce from the previous question that the distribution of the
    samples accepted by the above algorithm is $\pi$.}

    We know that given $X = x$, the probability of accepting $X$ is
    $\dfrac{\pi(x)}{q(x)}\dfrac{Z_{\pi}}{MZ_q}$. Therefore, 
    $$
    \pr(\text{Accept } X) = \int_{\mathbb{X}} \dfrac{\pi(x)}{q(x)}\dfrac{Z_{\pi}}{MZ_q} q(x) \, dx \\
    = \frac{Z_{\pi}}{MZ_q} 
    $$
    what implies that, by Bayes Theorem, the density of $X = x$ given that $X$
   was accepted is 
    $$
    \frac{\pi(x)}{q(x)}\dfrac{Z_{\pi}}{MZ_q}\frac{q(x)}{Z_{\pi}/(MZ_q)} = \pi(x).
    $$

    \item {\it Show that the probability that step (c) has to be carried out is}
    $$
    1 - \frac{\int_{\mathbb{X}} h(x) \, dx}{MZ_q}
    $$

    This probability can be written as 
    $$
    \pr\left(U > h(X) / (M\tilde{q}(X))\right) = 1 - \int_{\mathbb{X}} \frac{h(x)}{M\tilde{q}(x)} q(x) \, dx = 1 - \frac{\int_{\mathbb{X}} h(x) \, dx}{MZ_q}.
    $$

    \item {\it Let $\tilde{\pi}(x) = \exp(-x^2/2)$ and $\tilde{q}(x) =
    \exp(-|x|)$. Using the fact that }
    $$
    \tilde{\pi}(x) \ge 1 - \frac{x^2}{2}
    $$
    {\it 
    for any $x \in \R$, how could you use the squeeze rejection sampling
    algorithm to sample from $\pi(x)$. What is the probability of not having
    to evaluate $\tilde{\pi}(x)$? Why could it be beneficial to use this
    algorithm instead of the standard rejection sampling procedure?}

    Define $h(x) = \max(1 - x^2/2, 0)$. By the fact given above,
    $\tilde{\pi}(x) \ge h(x)$ for any $x \in \R$. Now, note that, 
    $$
    \sup_{x \in \R} \frac{\tilde{\pi}(x)}{\tilde{q}(x)} = \sup_{x \in \R} \exp(-x^2/2 + |x|).
    $$
    In order to maximize the above expression, suppose $x < 0$ is a
    local extreme, then 
    $$
    (-x - 1)e^{-x^2/2 - x} = 0 \implies x = -1. 
    $$
    Suppose now that $x > 0$ is a local extreme, then 
    $$
    (-x + 1)e^{-x^2/2 - x} = 0 \implies x = 1. 
    $$
    So the global maximum is attained at $x = -1$, $x = 0$ or $x = 1$. We have
    that 
    $$
    \sqrt{e} = \exp(-(-1)^2/2 + |-1|) = \exp(-1^2/2 + |1|) > \exp(0) = 1.
    $$
    Therefore, $\sup_{x \in \R} \tilde{\pi}(x)/\tilde{q}(x) = \sqrt{e}$. 
    Then, we have that 
    $$
    h(x) \le \tilde{\pi}(x) \le \sqrt{e} \tilde{q}(x)
    $$
    and we could use the  squeeze rejection sampling algorithm. 

    The probability of not having to evaluate $\tilde{\pi}(x)$ is the
    probability of accepting $X$ in step (b) that is 
    $$
    \int_{\R} \frac{h(x)}{M\tilde{q}(x)}q(x) \, dx = \frac{1}{\sqrt{e}Z_{q}}\int_{\R} h(x) \, dx = \frac{1}{\sqrt{e} Z_q} \int_{-\sqrt{2}}^{\sqrt{2}} 1 - \frac{x^2}{2} \, dx = \frac{2\sqrt{2}}{3\sqrt{e}} \approx 0.57, 
    $$
    since $Z_q = 2$. We have that calculating $h$ is simpler than $f$ and in
    half operations, we won't need to calculate it.  
\end{enumerate}

\begin{exercise}
    (Transformation)
    Consider the following algorithm known as Marsaglia's polar method.
    \begin{enumerate}[{\bf Step (a)}] 
    \item Generate independent $U_1, U_2$
    uniformly in $[-1,1]$ until $Y = U_1^2 + U_2^2 \le 1$.  
    \item Define $Z = \sqrt{-2\log(Y)}$ and return $X_i = Z U_i/\sqrt{Y}$ for $i=1,2$.
    \end{enumerate}
\end{exercise}

\begin{enumerate}
    \item {\it Define $\vartheta = \arctan(U_2/U_1)$. Show that the joint
    distribution of $Y$ and $\vartheta$ has density 
    $$
    f_{Y, \vartheta}(y,\theta) = 1_{[0,1]}(y)\frac{1_{[0,2\pi]}(\theta)}{2\pi}
    $$}
    Consider the transformation
    $$
    g(u_1, u_2) = \left(u_1^2 + u_2^2, \arctan(u_2/u_1)\right).
    $$ 
    The Jacobian of this transformation is 
    $$
    \begin{bmatrix}
        2u_1 & 2u_2 \\ 
        -u_2/(u_1^2 + u_2^2) & u_1/(u_1^2 + u_2^2)
    \end{bmatrix}
    $$
    and its determinant is $2$. Therefore, by the Change of Variable formula,
    since $u_2/u_1$ has image in $(-\infty, +\infty)$, we know that $\theta \in (-\pi/2,
    \pi/2)$. Besides that, is clear that $y \in [0,1]$. Therefore, 
    $$
    f_{Y, \vartheta}(y,\theta) = \frac{1}{8\pi/4}1_{\{\sqrt{y}(\cos(\theta), \sin(\theta)) \in [-1,1]^2\}}1_{\{y \le 1\}} = \frac{1}{2\pi}1_{[0,1]}(y)1_{[0,2\pi]}(\theta).
    $$

    \item {\it Show that $X_1$ and $X_2$ are independent standard normal random variables.}
    
    Putting $(U_1, U_2) = \sqrt{Y}(\cos(\vartheta), \sin(\vartheta))$, we have
    that 
    $$X_1 = ZU_1/\sqrt{Y} = \sqrt{-2\log(Y)}\cos(\vartheta), \quad X_2 =
    \sqrt{-2\log(Y)}\sin(\vartheta).$$
    Then, $(X_1, X_2)$ is a transformation of $(Y,\vartheta)$ which have uniform
    distribution over $[0,1] \times [0, 2\pi]$. The Jacobian of this
    distribution is 
    $$
    \begin{bmatrix}
        \frac{-2}{2y\sqrt{-2\log(y)}}\cos(\theta) & -\sqrt{-2\log(y)}\sin(\theta) \\
        \frac{-2}{2y\sqrt{-2\log(y)}}\sin(\theta) & \sqrt{-2\log(y)}\cos(\theta),
    \end{bmatrix}
    $$
    whose determinant is 
    $$\frac{-\cos^2(\theta) - \sin^2(\theta)}{y} = -\frac{1}{y}.$$
    Note that $X_1^2 + X_2^2 = -2\log(Y) \implies Y =
    \exp\left\{-\frac{1}{2}(X_1^2 + X_2^2)\right\}$
     The density of the distribution of $(X_1, X_2)$ is 
    $$
    f_{X_1, X_2}(x_1, x_2) = \frac{1}{2\pi} e^{-\frac{1}{2}(x_1^2 + x_2^2)} = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x_1^2} \cdot \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x_2^2},
    $$
    which implies that $X_1, X_2 \overset{iid}{\sim}
    \operatorname{Normal}(0,1)$.
    
    \item {\it  What are the potential benefits of this approach over the Box-Muller algorithm?}
    
    The main beneficial part is that is not necessary to calculate any
    trigonometric functions, which are more expensive that logarithm. 

\end{enumerate}

\begin{exercise}
    
\end{exercise}

\begin{exercise}[Rejection and Importance Sampling]
    Consider two probability densities $\pi$, $q$ on $\mathbb{X}$ such that
    $\pi(x) > 0 \implies q(x) > 0$ and assume that you can easily draw samples
    from $q$. Whenever $\pi(x)/q (x) \le M < \infty$ for any $x \in
    \mathbb{X}$, it is possible to use rejection sampling to sample from
    $\pi$. When $M$ is unknown or when this condition is not satisfied, we can
    use importance sampling techniques to approximate expectations 
    with respect to $\pi$. However it might be the case that most 
    samples from $q$ have very small importance weights. 
    
    Rejection control is a method combining rejection and 
    importance weighting. It relies on an arbitrary threshold 
    value $c > 0$. We introduce the notation $w (x) = \pi (x) /q (x)$ 
    and
    $$
    Z_c = \int_{\mathbb{X}} \min\{{1, w(x) / c}\} q(x) \, dx.
    $$
    Rejection control proceeds as follows. 
    \begin{itemize}
        \item {\bf Step a.} Generate independent $X \sim q$, 
        $U \in \unif[0,1]$ until $U \le \min\{{1, w (X) /c }\}$.
        \item {\bf Step b.} Return $X$.  
    \end{itemize}
\end{exercise}

\begin{enumerate}
    \item {\it Give the expression of the probability density 
    $q^*(x)$ of the accepted samples.}

    Notice that if $A \subseteq \mathbb{X}$ is measurable, 
    \begin{equation*}
        \begin{split}
            \pr(X \in A, X \text{ accepted}) &= \int_{\mathbb{X}} \int_0^{\min(1, w(x)/c)} 1_A(x) q(x) \, du \, dx \\
            &= \int_{\mathbb{X}} 1_A(x) \min(1, w(x)/c) q(x) \, dx. 
        \end{split}
    \end{equation*}
    Besides that, $\pr(X \text{ accepted}) = Z_c$ using the aboce expression
    with $A = \mathbb{X}$. Therefore, $q^*(x) = Z_c^{-1}\min(1, w(x)/c) q(x) =
    Z_c^{-1} \min(q(x), \pi(x)/ c)$. 

    \item Prove that 
    $$
    \ev_{q^*}\left([w^*(X)]^2\right) = Z_c\ev_q(\max\{{w(X), c}\}w(X)),
    $$
    where $w^*(x) = \pi(x) / q^*(x)$. 

    The left side of the equation is 
    $$
    I_1 = \int_{\mathbb{X}} w^*(x)^2 q^*(x) \, dx = Z_c\int_{\mathbb{X}} \frac{\pi(x)^2}{\min(q(x), \pi(x)/ c)} \, dx,
    $$
    while the right side is 
    $$
    I_2 = Z_c\int_{\mathbb{X}} \max(w(x), c)w(x)q(x) \, dx = Z_c\int_{\mathbb{X}} \max\left(\frac{\pi^2(x)}{q(x)}, c\pi(x)\right)\, dx 
    $$
    Define $X_1 = \{x \in \mathbb{X} \mid cq(x) \le \pi(x)\}$ and $X_2 =
    \mathbb{X} / X_1$. Then
    $$
    I_1 = Z_c\int_{X_1} \frac{\pi(x)^2}{q(x)} \, dx + Z_c\int_{X_2} c\pi(x) \, dx,,
    $$
    and 
    $$
    I_2 = Z_c\int_{X_1} \frac{\pi^2(x)}{q(x)} \, dx + Z_c\int_{X_2} c\pi(x)\, dx, 
    $$
    which implies that $I_1 = I_2$ as claimed. 

    \item Establish that
    $$
    \ev_q(\min\{{w(X), c}\})\ev_q(\max\{{w (X), c}\} 
    w(x)) \le \ev_q(\min\{{w (X) , c}\}\max\{{w(X), c}\}w (X))
    $$

    First, let's prove that 
    $$h(w_1, w_2) = [\min\{w_1, c\} - \min\{w_2, c\}] [w_1 \max \{w_1, c\} -
    w_2 \max\{w_2, c\}] \ge 0$$      
    There are three cases: 
    \begin{enumerate}[(i)]
        \item $w_1, w_2 \le c$: In this case, $h(w_1, w_2) = (w_1 - w_2)c(w_1 -
        w_2) = c(w_1 - w_2) \ge 0$. 
        \item $w_1, w_2 \ge c$: In this case the first factor is zero and
        $h(w_1, w_2) = 0$. 
        \item $w_1 < c < w_2$: In this case, $h(w_1, w_2) = (w_1 - c)(cw_1 -
        w_2^2) = (c - w_1)(w_2^2 - cw_1) > 0$, given that $w_2^2 > c^2 >
        cw_1$, supposing $w_1 \ge 0$. Notice that $w_2 < c < w_1$ is
        analogous.   
    \end{enumerate}

    Alongside this result, we see that for every realization of $X$
    $x_1, x_2$, we have that $h(w(x_1), w(x_2)) \ge 0$, which implies 
    that the random variables $\min(w(X), c)$ and $\max(w(X), c)w(X)$
    are positively correlated. The claimed result follows. 

    \item Deduce from the results established in (2) and (3) that
    $$
    \var_{q^*}(w^*(X)) \le \var_q(w(X))
    $$

    First, notice that $\ev_{q^*}(w^*(X)) = \int_{\mathbb{X}} \pi(x) \, dx  =
    1$ and  $\ev_{q}(w(X)) = \int_{\mathbb{X}} \pi(x) \, dx  =
    1$

    We have that 
    \begin{equation*}
        \begin{split}
            c\left(1 + \var_{q^*}(w^*(X))\right) &=  c\ev_{q^*}\left([w^*(X)]^2\right) \\
            &= cZ_c\ev_q(\max\{{w(X), c}\}w(X)) s\textcolor{blue}{\quad(2)}\\
            &= c\ev_q(\min(1, w(X) / c))\ev_q(\max\{{w(X), c}\}w(X)) \\
            &= \ev_q(\min(c, w(X)))\ev_q(\max\{{w(X), c}\}w(X)) \\
            &\le \ev_q\left(\min(c,w(X)) \max(w(X), c) w(X)\right)  \textcolor{blue}{\quad(3)}\\
            &= \int_{\mathbb{X}} \min(c,w(x))\max(c,w(x))w(x)q(x) \, dx \\
            &= c\int_{\mathbb{X}} w^2(x) q(x) \, dx \\ 
            &= c\ev_q(w(X)^2) \\
            &= c\left(1 + \var_{q}(w(X))\right), 
        \end{split}
    \end{equation*}
    what implies the desired result. 


\end{enumerate}

\begin{exercise}
    
\end{exercise}

% \bibliographystyle{apalike}
% \bibliography{../stat_comp}

\end{document}          
